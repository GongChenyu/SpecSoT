"""
SP+PPåˆ†å¸ƒå¼æ¨ç†ä¸»è„šæœ¬
æ”¯æŒä¸‰å°è®¾å¤‡çš„Sequence Parallel + Pipeline Parallelæ¨ç†
Prefillé˜¶æ®µï¼šSP(chunk_size=128) + PP(æ¨¡å‹å±‚å‡åˆ†)
Decodeé˜¶æ®µï¼šå…¨é‡å†—ä½™è®¡ç®—
"""

import os
import time
import argparse
import torch
import torch.distributed as dist
from typing import List, Optional, Tuple
from transformers import AutoTokenizer
from modeling_qwen3_kv_distributed import Qwen3ForCausalLMDistributed
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class DistributedInferenceEngine:
    def __init__(
        self,
        model_path: str,
        rank: int,
        world_size: int,
        master_addr: str,
        master_port: str,
        chunk_size: int = 128,
        sync_strategy: str = "pairwise",  # "pairwise" or "ring"
        device_mode: str = "single_node",  # "single_node" or "multi_node"
        backend: str = "auto",  # "nccl", "gloo", or "auto"
    ):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼æ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            rank: å½“å‰è®¾å¤‡rank (0, 1, 2)
            world_size: æ€»è®¾å¤‡æ•° (3)
            master_addr: ä¸»èŠ‚ç‚¹åœ°å€
            master_port: ä¸»èŠ‚ç‚¹ç«¯å£
            chunk_size: SPçš„chunkå¤§å°
            sync_strategy: cacheåŒæ­¥ç­–ç•¥ ("pairwise"æˆ–"ring")
            device_mode: è®¾å¤‡æ¨¡å¼ ("single_node"å•æœºå¤šå¡ æˆ– "multi_node"å¤šæœºå•å¡)
            backend: é€šä¿¡åç«¯ ("nccl", "gloo", æˆ– "auto"è‡ªåŠ¨é€‰æ‹©)
        """
        self.rank = rank
        self.world_size = world_size
        self.chunk_size = chunk_size
        self.sync_strategy = sync_strategy
        self.model_path = model_path
        self.device_mode = device_mode
        self.backend_preference = backend

        # åˆå§‹åŒ–logger
        self.logger = self._setup_logger()
        
        # è®¾ç½®æœ¬åœ°è®¾å¤‡
        self.local_device = self._get_local_device()
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        self._init_distributed(master_addr, master_port)
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self.logger.info(f"åˆå§‹åŒ–è®¾å¤‡ Rank {rank}/{world_size}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = self._load_model()
        
        # æ—¶é—´æµ‹é‡ç‚¹
        self.timing_stats = {
            'prefill_start': 0,
            'prefill_end': 0,
            'cache_sync_end': 0,
            'decode_start': 0,
            'decode_end': 0
        }
        
        # ğŸ†• Cacheæ¥æ”¶çŠ¶æ€è¿½è¸ªçŸ©é˜µ: [num_chunks, num_pp_stages]
        # åˆå§‹åŒ–æ—¶ä¸çŸ¥é“chunkæ•°é‡ï¼Œåœ¨prefillé˜¶æ®µåŠ¨æ€åˆ›å»º
        self.cache_received_indicator = None
        self.num_chunks = 0
        self.num_pp_stages = world_size  # PP stageæ•°é‡ç­‰äºworld_size
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(f"Rank-{self.rank}")
        logger.setLevel(logging.INFO)
        return logger
    
    def _get_local_device(self) -> int:
        """
        æ ¹æ®è®¾å¤‡æ¨¡å¼è·å–æœ¬åœ°CUDAè®¾å¤‡ID
        """
        # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº† CUDA_VISIBLE_DEVICES
        if "CUDA_VISIBLE_DEVICES" in os.environ:
            # å¦‚æœç¯å¢ƒå˜é‡é™åˆ¶äº†å¯è§æ˜¾å¡ï¼ˆä¾‹å¦‚ Bash è„šæœ¬ä¸­è®¾ç½®äº†ï¼‰ï¼Œ
            # é‚£ä¹ˆæ— è®ºç‰©ç†IDæ˜¯å¤šå°‘ï¼Œå½“å‰è¿›ç¨‹å†…åªèƒ½çœ‹åˆ°è¢«æ˜ å°„ä¸º 0 çš„è®¾å¤‡ã€‚
            # è¿™ç§æƒ…å†µä¸‹ï¼ŒRank 1 çš„è¿›ç¨‹çœ‹åˆ°çš„ä¹Ÿæ˜¯ cuda:0
            visible_devices = os.environ["CUDA_VISIBLE_DEVICES"]
            self.logger.info(f"[Rank {self.rank}] æ£€æµ‹åˆ° CUDA_VISIBLE_DEVICES={visible_devices}ï¼Œä½¿ç”¨é€»è¾‘è®¾å¤‡ cuda:0")
            return 0
            
        if self.device_mode == 'multi_node':
            # å¤šæœºå•å¡æ¨¡å¼ï¼šæ¯å°æœºå™¨ä½¿ç”¨GPU 0
            self.logger.info(f"[Rank {self.rank}] å¤šæœºå•å¡æ¨¡å¼ï¼Œä½¿ç”¨ç‰©ç† GPU 0ï¼ˆé€»è¾‘ cuda:0ï¼‰")
            return 0
        else:  # single_node
            # å¦‚æœæ²¡æœ‰è®¾ç½® CUDA_VISIBLE_DEVICESï¼ˆæ¯”å¦‚ç›´æ¥ python script.py å¯åŠ¨ï¼‰ï¼Œ
            # é‚£ä¹ˆè¿›ç¨‹èƒ½çœ‹åˆ°æ‰€æœ‰å¡ï¼Œæ­¤æ—¶éœ€è¦ç”¨ rank æ¥æŒ‡å®šå…·ä½“ç”¨å“ªå¼ å¡ã€‚
            self.logger.info(f"[Rank {self.rank}] å•æœºå¤šå¡æ¨¡å¼(æ— ç¯å¢ƒéš”ç¦»)ï¼Œä½¿ç”¨GPU {self.rank}")
            return self.rank
        
    def _init_distributed(self, master_addr: str, master_port: str):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        os.environ['MASTER_ADDR'] = master_addr
        os.environ['MASTER_PORT'] = master_port
        os.environ['RANK'] = str(self.rank)
        os.environ['WORLD_SIZE'] = str(self.world_size)
        
        # é€‰æ‹©é€šä¿¡åç«¯
        if self.backend_preference == 'auto':
            # è‡ªåŠ¨é€‰æ‹©
            # å•æœºå¤šå¡ï¼šä½¿ç”¨ncclï¼ˆæ›´å¿«ï¼Œæ”¯æŒCUDA tensorsç›´æ¥é€šä¿¡ï¼‰
            # å¤šæœºç¯å¢ƒï¼ˆå°¤å…¶æ˜¯æ— çº¿è¿æ¥çš„Jetsonï¼‰ï¼šä½¿ç”¨glooï¼ˆæ›´ç¨³å®šï¼Œæ”¯æŒTCPï¼Œä½†åªèƒ½å‘é€CPU tensorsï¼‰
            if self.device_mode == 'single_node':
                backend = 'nccl'
                self.logger.info(f"è‡ªåŠ¨é€‰æ‹©ï¼šå•æœºå¤šå¡ç¯å¢ƒï¼Œä½¿ç”¨NCCLåç«¯")
            else:
                backend = 'gloo'
                self.logger.info(f"è‡ªåŠ¨é€‰æ‹©ï¼šå¤šæœºç¯å¢ƒï¼Œä½¿ç”¨Glooåç«¯ï¼ˆé€‚åˆTCP/æ— çº¿ç½‘ç»œï¼‰")
        else:
            # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åç«¯
            backend = self.backend_preference
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šåç«¯: {backend}")
        
        # ä¿å­˜å®é™…ä½¿ç”¨çš„backend
        self.backend = backend
        
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        dist.init_process_group(
            backend=backend,
            init_method=f'tcp://{master_addr}:{master_port}',
            rank=self.rank,
            world_size=self.world_size
        )
        
        # è®¾ç½®å½“å‰è®¾å¤‡ï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„æœ¬åœ°è®¾å¤‡ï¼‰
        torch.cuda.set_device(self.local_device)
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå…¨é‡æ¨¡å‹ï¼‰"""
        self.logger.info(f"åŠ è½½æ¨¡å‹: {self.model_path}")
        self.logger.info(f"æ³¨æ„ï¼šæ‰€æœ‰è®¾å¤‡éƒ½åŠ è½½å®Œæ•´æ¨¡å‹ï¼ŒPrefillé˜¶æ®µé€‰æ‹©å¯¹åº”å±‚è®¡ç®—")
        
        model = Qwen3ForCausalLMDistributed.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map=f"cuda:{self.local_device}",
            rank=self.rank,
            world_size=self.world_size,
            sync_strategy=self.sync_strategy,
            backend=self.backend
        )
        model.eval()        
        self.logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå…±{model.config.num_hidden_layers}å±‚")
        
        # ğŸ†• åˆå§‹åŒ–é¢„åˆ†é…çš„KV Cacheï¼ˆè¿ç»­å†…å­˜ï¼‰
        max_seq_length = 2200  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        model.model.initialize_kv_cache(max_length=max_seq_length, batch_size=1)
        self.logger.info(f"KV Cache é¢„åˆ†é…å®Œæˆï¼ˆæœ€å¤§é•¿åº¦: {max_seq_length}ï¼‰")
        
        return model
        
    def _split_prompt_chunks(self, input_ids: torch.Tensor) -> List[torch.Tensor]:
        """å°†promptæŒ‰chunk_sizeåˆ‡åˆ†"""
        seq_len = input_ids.shape[1]
        chunks = []
        for i in range(0, seq_len, self.chunk_size):
            chunk = input_ids[:, i:i+self.chunk_size]
            chunks.append(chunk)
        return chunks
        
    def _get_layer_range(self, num_layers: int) -> Tuple[int, int]:
        """è·å–å½“å‰rankè´Ÿè´£çš„å±‚èŒƒå›´"""
        layers_per_device = num_layers // self.world_size
        start_layer = self.rank * layers_per_device
        end_layer = start_layer + layers_per_device if self.rank < self.world_size - 1 else num_layers
        return start_layer, end_layer
        
    def prefill_phase(self, prompt: str) -> Tuple[torch.Tensor, List]:
        """
        Prefillé˜¶æ®µï¼šä½¿ç”¨SP+PP
        
        Returns:
            last_hidden_state: æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
            kv_caches: æ‰€æœ‰å±‚çš„KV cache
        """
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹ Prefill é˜¶æ®µ (SP+PP)")
        self.timing_stats['prefill_start'] = time.time()
        
        # ğŸ†• é‡ç½®KV Cacheç¼“å†²åŒº
        self.model.model.reset_kv_cache()
        self.logger.info("KV Cache å·²é‡ç½®")
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(f"cuda:{self.local_device}")
        input_ids = inputs['input_ids']
        
        self.logger.info(f"Prompté•¿åº¦: {input_ids.shape[1]} tokens")
        
        # åˆ‡åˆ†chunks
        chunks = self._split_prompt_chunks(input_ids)
        self.logger.info(f"åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªchunks (chunk_size={self.chunk_size})")
        
        # ğŸ†• åˆå§‹åŒ–Cacheæ¥æ”¶çŠ¶æ€è¿½è¸ªçŸ©é˜µ: [num_chunks Ã— num_layers]
        self.num_chunks = len(chunks)
        num_layers = self.model.config.num_hidden_layers
        self.cache_received_indicator = torch.zeros(
            (self.num_chunks, num_layers),
            dtype=torch.int8,
            device='cpu'
        )
        self.logger.info(f"åˆå§‹åŒ–Cacheæ¥æ”¶çŠ¶æ€çŸ©é˜µ: [{self.num_chunks} chunks Ã— {num_layers} layers]")
        
        # è·å–å½“å‰rankè´Ÿè´£çš„å±‚èŒƒå›´
        start_layer, end_layer = self._get_layer_range(num_layers)
        self.logger.info(f"è´Ÿè´£å±‚èŒƒå›´: [{start_layer}, {end_layer})")
        
        # è®¾ç½®æ¨¡å‹çš„PPèŒƒå›´
        self.model.set_pipeline_range(start_layer, end_layer)
        
        # ğŸ†• ä½¿ç”¨é¢„åˆ†é…çš„KV Cacheï¼ˆå·²åœ¨_load_modelä¸­åˆå§‹åŒ–ï¼‰
        # è·å–æ¨¡å‹çš„KV cacheå¯¹è±¡
        kv_cache_list = self.model.model.past_key_values
        self.logger.info(f"ä½¿ç”¨é¢„åˆ†é…çš„KV Cacheï¼Œå…±{len(kv_cache_list)}å±‚")
        
        # é€chunkã€é€å±‚å¤„ç†
        last_hidden = None
        
        for chunk_idx, chunk in enumerate(chunks):
            self.logger.info(f"å¤„ç† chunk {chunk_idx+1}/{len(chunks)}")
            
            # ç¬¬ä¸€ä¸ªrankä»embeddingå¼€å§‹
            if self.rank == 0:
                hidden_states = self.model.model.embed_tokens(chunk)
                self.logger.debug(f"  Rank 0: ç”Ÿæˆembedding, shape={hidden_states.shape}")
                batch_size, seq_length = chunk.shape
            else:
                # å…¶ä»–rankç¨ååœ¨å±‚å¾ªç¯ä¸­æ¥æ”¶hidden states
                hidden_states = None
                batch_size, seq_length = 1, chunk.shape[1]   # è¿™é‡Œä¸å¤Ÿé²æ£’
            
            # è®¡ç®—past_key_values_length
            past_key_values_length = 0
            if kv_cache_list[0][0].current_length > 0:
                past_key_values_length = kv_cache_list[0][0].current_length.item()
            
            seq_length_with_past = seq_length + past_key_values_length
            
            # ğŸ†• éå†æ‰€æœ‰36å±‚ï¼šå½“å‰rankè®¡ç®—è‡ªå·±è´Ÿè´£çš„å±‚ï¼Œæ‰€æœ‰rankå‚ä¸cacheåŒæ­¥
            for layer_idx in range(num_layers):
                # ğŸ”„ åœ¨å½“å‰rankçš„ç¬¬ä¸€å±‚ä¹‹å‰ï¼Œæ¥æ”¶ä¸Šä¸€ä¸ªrankçš„hidden states
                if layer_idx == start_layer and self.rank > 0:
                    hidden_states = self._receive_hidden_states()
                    batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]
                    seq_length_with_past = seq_length + past_key_values_length
                    self.logger.debug(f"  Rank {self.rank}: åœ¨layer {layer_idx}å‰æ¥æ”¶hidden states, shape={hidden_states.shape}")
                
                # åˆ¤æ–­å½“å‰rankæ˜¯å¦è´Ÿè´£è¿™ä¸€å±‚
                if start_layer <= layer_idx < end_layer:
                    # æˆ‘è´Ÿè´£è¿™ä¸€å±‚ï¼Œè¿›è¡Œè®¡ç®—
                    self.logger.debug(f"  è®¡ç®—å±‚ {layer_idx}")
                    
                    # å‡†å¤‡attention maskå’Œposition idsï¼ˆåœ¨ç¬¬ä¸€å±‚æ—¶ï¼‰
                    if layer_idx == start_layer:
                        position_ids = torch.arange(
                            past_key_values_length,
                            seq_length + past_key_values_length,
                            dtype=torch.long,
                            device=f"cuda:{self.local_device}",
                        ).unsqueeze(0)
                        
                        attention_mask = torch.ones(
                            (batch_size, seq_length_with_past),
                            dtype=torch.bool,
                            device=f"cuda:{self.local_device}",
                        )
                        
                        attention_mask = self.model.model._prepare_decoder_attention_mask(
                            attention_mask,
                            (batch_size, seq_length),
                            hidden_states,
                            past_key_values_length,
                        )
                        
                        position_embeddings = self.model.model.rotary_emb(hidden_states, position_ids)
                    
                    # è·å–è¯¥å±‚çš„past_key_valueï¼ˆä½¿ç”¨KVCacheå¯¹è±¡ï¼‰
                    layer_past_kv = kv_cache_list[layer_idx] if past_key_values_length > 0 else None
                    
                    # å•å±‚forward
                    layer_output = self.model.forward_single_layer(
                        layer_idx=layer_idx,
                        hidden_states=hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        position_embeddings=position_embeddings,
                        past_key_value=layer_past_kv,
                        use_cache=True
                    )
                    
                    hidden_states = layer_output['hidden_states']
                    new_kv_cache = layer_output['past_key_value']
                else:
                    # æˆ‘ä¸è´Ÿè´£è¿™ä¸€å±‚ï¼Œè·³è¿‡è®¡ç®—
                    new_kv_cache = None
                
                # ğŸ†• æ‰€æœ‰rankå‚ä¸è¯¥å±‚cacheçš„å¹¿æ’­åŒæ­¥ï¼Œå¹¶ä½¿ç”¨KVCache.append_sequence()è¿½åŠ 
                self._sync_and_append_cache(layer_idx, new_kv_cache, chunk_idx, kv_cache_list)
                
                # ğŸ†• æ ‡è®°è¯¥å±‚cacheå·²æ¥æ”¶
                self.cache_received_indicator[chunk_idx, layer_idx] = 1
                
                # ğŸ”„ åœ¨å½“å‰rankçš„æœ€åä¸€å±‚ä¹‹åï¼Œå‘é€hidden statesç»™ä¸‹ä¸€ä¸ªrank
                if layer_idx == end_layer - 1 and self.rank < self.world_size - 1:
                    self._send_hidden_states(hidden_states)
                    self.logger.debug(f"  Rank {self.rank}: åœ¨layer {layer_idx}åå‘é€hidden statesåˆ° Rank {self.rank+1}")
            
            # å½“å‰chunkåœ¨å½“å‰rankçš„æ‰€æœ‰å±‚è®¡ç®—å®Œæˆ
            # æœ€åä¸€ä¸ªrankè¿‡norm
            if self.rank == self.world_size - 1:
                hidden_states = self.model.model.norm(hidden_states)
                self.logger.debug(f"  Rank {self.world_size-1}: åº”ç”¨norm")
            
            last_hidden = hidden_states  # ä¿å­˜æœ€åçš„hidden_states
            
            self.logger.info(f"  chunk {chunk_idx+1} å¤„ç†å®Œæˆ")
        
        self.timing_stats['prefill_end'] = time.time()
        prefill_time = self.timing_stats['prefill_end'] - self.timing_stats['prefill_start']
        self.logger.info(f"Prefill å®Œæˆï¼Œè€—æ—¶: {prefill_time:.3f}s")
        
        # ğŸ†• æ‰“å°Cacheæ¥æ”¶çŠ¶æ€çŸ©é˜µ
        self._log_cache_received_status()
        
        try:
            torch.cuda.synchronize()
            dist.barrier()
        except Exception as e:
            self.logger.error(f"åŒæ­¥å¤±è´¥: {e}")
            # åˆ·æ–°æ—¥å¿—
            for handler in self.logger.handlers:
                handler.flush()
            raise
        
        self.timing_stats['cache_sync_end'] = self.timing_stats['prefill_end']
        self.logger.info(f"Cache å·²åœ¨è®¡ç®—è¿‡ç¨‹ä¸­é€å±‚åŒæ­¥å®Œæˆ")
        
        # è¿”å›æ‰€æœ‰å±‚çš„cacheï¼ˆdecodeé˜¶æ®µéœ€è¦å®Œæ•´çš„36å±‚cacheï¼‰
        # ç›´æ¥è¿”å›KVCacheå¯¹è±¡åˆ—è¡¨
        self.logger.info(f"Prefillå®Œæˆï¼Œè¿”å›{len(kv_cache_list)}å±‚KVCacheå¯¹è±¡")
        if kv_cache_list[0][0].current_length > 0:
            cache_len = kv_cache_list[0][0].current_length.item()
            self.logger.info(f"Cacheç¤ºä¾‹ - Layer 0: current_length={cache_len}, shape={kv_cache_list[0][0].shape}")
        
        return last_hidden, kv_cache_list
        
    def decode_phase(self, last_hidden: torch.Tensor, kv_caches: List, max_new_tokens: int = 100) -> str:
        """
        Decodeé˜¶æ®µï¼šæ‰€æœ‰è®¾å¤‡è¿›è¡Œç›¸åŒçš„å…¨é‡è®¡ç®—
        
        Args:
            kv_caches: Prefillé˜¶æ®µç”Ÿæˆçš„KV cache
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            generated_text: ç”Ÿæˆçš„æ–‡æœ¬
        """
        # ç­‰å¾…prefillå’ŒcacheåŒæ­¥éƒ½å®Œæˆ
        try:
            dist.barrier()
        except Exception as e:
            self.logger.error(f"Decode é˜¶æ®µåŒæ­¥å¤±è´¥: {e}")
            for handler in self.logger.handlers:
                handler.flush()
            raise
        
        self.logger.info("=" * 60)
        self.logger.info("å¼€å§‹ Decode é˜¶æ®µ (å…¨é‡å†—ä½™è®¡ç®—)")
        self.timing_stats['decode_start'] = time.time()
        
        # é‡ç½®æ¨¡å‹ä¸ºå…¨é‡æ¨¡å¼
        self.model.set_full_model_mode()
        
        # è·å–ä¸Šä¸€ä¸ªtoken (ä»prefillçš„æœ€åè¾“å‡º)
        # ä»kv_cacheæ¨æ–­å½“å‰ä½ç½®ï¼ˆä½¿ç”¨KVCacheçš„current_lengthï¼‰
        current_position = kv_caches[0][0].current_length.item()
        self.logger.info(f"Decodeå¼€å§‹æ—¶çš„cacheé•¿åº¦: {current_position}")
        
        # ç”Ÿæˆåˆå§‹token - åªæœ‰æœ€åä¸€ä¸ªrankè®¡ç®—ï¼Œç„¶åå¹¿æ’­ç»™æ‰€æœ‰rank
        if self.rank == self.world_size - 1:
            # æœ€åä¸€ä¸ªè®¾å¤‡æœ‰å®Œæ•´çš„hidden statesï¼Œä»last_hiddenç”Ÿæˆç¬¬ä¸€ä¸ªtoken
            # éœ€è¦è¿‡lm_headå¾—åˆ°logits
            with torch.no_grad():
                logits = self.model.lm_head(last_hidden[:, -1:, :])  # [batch, 1, vocab_size]
                next_token_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)  # [batch, 1]
        else:
            next_token_id = None
        
        # å¹¿æ’­åˆå§‹tokenåˆ°æ‰€æœ‰rank
        next_token_id = self._broadcast_next_token(next_token_id)
        self.logger.debug(f"åˆå§‹token: {next_token_id.item()}")
        
        generated_tokens = [next_token_id.item()]
        
        # è‡ªå›å½’ç”Ÿæˆ
        for step in range(max_new_tokens):
            # æ‰€æœ‰è®¾å¤‡æ‰§è¡Œç›¸åŒçš„forward
            # å°†KVCacheå¯¹è±¡è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼ï¼ˆtuple of tensorsï¼‰
            past_kv_tuples = []
            for layer_kv in kv_caches:
                key_cache = layer_kv[0].get_data()
                value_cache = layer_kv[1].get_data()
                past_kv_tuples.append((key_cache, value_cache))
            
            with torch.no_grad():
                outputs = self.model(
                    input_ids=next_token_id,
                    past_key_values=past_kv_tuples,
                    use_cache=True
                )
            
            # æ›´æ–°KVCacheï¼šå°†æ¨¡å‹è¾“å‡ºçš„æ–°cacheè¿½åŠ åˆ°KVCacheå¯¹è±¡ä¸­
            new_past_kv = outputs.past_key_values
            for layer_idx in range(len(kv_caches)):
                new_key, new_value = new_past_kv[layer_idx]
                # åªè¿½åŠ æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆæœ€åä¸€ä¸ªtokençš„cacheï¼‰
                new_key_slice = new_key[:, :, -1:, :]  # [batch, heads, 1, dim]
                new_value_slice = new_value[:, :, -1:, :]
                kv_caches[layer_idx][0].cat(new_key_slice, dim=2)
                kv_caches[layer_idx][1].cat(new_value_slice, dim=2)
            logits = outputs.logits[:, -1, :]  # [batch, vocab_size]
            
            # ç®€å•çš„greedy decoding
            next_token_id = torch.argmax(logits, dim=-1, keepdim=True)  # [batch, 1]
            
            generated_tokens.append(next_token_id.item())
            
            # æ¯5æ­¥æ‰“å°cacheé•¿åº¦ä»¥è°ƒè¯•
            if (step + 1) % 5 == 0:
                cache_len = kv_caches[0][0].current_length.item()
                self.logger.info(f"Step {step+1}: cacheé•¿åº¦={cache_len}, æœ€æ–°token={next_token_id.item()}")
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æŸtoken
            if next_token_id.item() == self.tokenizer.eos_token_id:
                self.logger.info(f"é‡åˆ°EOS tokenï¼Œåœæ­¢ç”Ÿæˆ (step {step+1})")
                break
                
            if (step + 1) % 10 == 0:
                self.logger.info(f"å·²ç”Ÿæˆ {step+1} tokens")
        
        self.timing_stats['decode_end'] = time.time()
        decode_time = self.timing_stats['decode_end'] - self.timing_stats['decode_start']
        self.logger.info(f"Decode å®Œæˆï¼Œè€—æ—¶: {decode_time:.3f}s")
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return generated_text
        
    def _send_hidden_states(self, hidden_states: torch.Tensor):
        """å‘é€hidden statesåˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡"""
        # Glooåç«¯éœ€è¦åœ¨CPUä¸Šé€šä¿¡
        if self.backend == 'gloo':
            # å…ˆå‘é€shapeä¿¡æ¯ï¼ˆåœ¨CPUä¸Šï¼‰
            shape = torch.tensor(hidden_states.shape, dtype=torch.long, device='cpu')
            dist.send(shape, dst=self.rank + 1)
            # å°†æ•°æ®ç§»åˆ°CPUå†å‘é€
            hidden_states_cpu = hidden_states.contiguous().cpu()
            dist.send(hidden_states_cpu, dst=self.rank + 1)
        else:
            # NCCLåç«¯å¯ä»¥ç›´æ¥åœ¨GPUä¸Šé€šä¿¡
            shape = torch.tensor(list(hidden_states.shape), dtype=torch.long, device=f"cuda:{self.local_device}")
            dist.send(shape, dst=self.rank + 1)
            dist.send(hidden_states.contiguous(), dst=self.rank + 1)
        
    def _receive_hidden_states(self) -> torch.Tensor:
        """ä»ä¸Šä¸€ä¸ªè®¾å¤‡æ¥æ”¶hidden states"""
        # Glooåç«¯éœ€è¦åœ¨CPUä¸Šé€šä¿¡
        if self.backend == 'gloo':
            # å…ˆæ¥æ”¶shapeä¿¡æ¯ï¼ˆåœ¨CPUä¸Šï¼‰
            shape = torch.zeros(3, dtype=torch.long, device='cpu')
            dist.recv(shape, src=self.rank - 1)
            self.logger.debug(f"  æ¥æ”¶åˆ°shape: {shape.tolist()}")
            
            # åœ¨CPUä¸Šåˆ›å»ºtensorå¹¶æ¥æ”¶æ•°æ®
            hidden_states_cpu = torch.zeros(
                tuple(shape.tolist()),
                dtype=torch.float16
            )
            dist.recv(hidden_states_cpu, src=self.rank - 1)
            # ç§»åˆ°GPU
            hidden_states = hidden_states_cpu.to(f"cuda:{self.local_device}")
            self.logger.debug(f"  æˆåŠŸæ¥æ”¶hidden stateså¹¶ç§»åˆ°GPU")
        else:
            # NCCLåç«¯å¯ä»¥ç›´æ¥åœ¨GPUä¸Šé€šä¿¡
            shape = torch.zeros(3, dtype=torch.long, device=f"cuda:{self.local_device}")
            dist.recv(shape, src=self.rank - 1)
            self.logger.debug(f"  æ¥æ”¶åˆ°shape: {shape.tolist()}")
            
            hidden_states = torch.zeros(
                tuple(shape.tolist()),
                dtype=torch.float16,
                device=f"cuda:{self.local_device}"
            )
            dist.recv(hidden_states, src=self.rank - 1)
            self.logger.debug(f"  æˆåŠŸæ¥æ”¶hidden states")
        
        return hidden_states
    
    def _sync_and_append_cache(self, layer_idx: int, local_cache, chunk_idx: int, kv_cache_list):
        """
        åŒæ­¥å¹¶è¿½åŠ cacheåˆ°KVCacheå¯¹è±¡
        
        Args:
            layer_idx: å±‚ç´¢å¼•
            local_cache: å½“å‰rankè®¡ç®—çš„cache (KVCacheå¯¹è±¡æˆ–None)
            chunk_idx: chunkç´¢å¼•
            kv_cache_list: KVCacheå¯¹è±¡åˆ—è¡¨
        """
        # ç¡®å®šå“ªä¸ªrankè´Ÿè´£è¿™ä¸€å±‚
        num_layers = self.model.config.num_hidden_layers
        layers_per_rank = num_layers // self.world_size
        owner_rank = layer_idx // layers_per_rank
        if owner_rank >= self.world_size:
            owner_rank = self.world_size - 1
        
        if self.rank == owner_rank:
            # æˆ‘æ˜¯ownerï¼Œå¹¿æ’­cache
            if local_cache is None:
                raise RuntimeError(f"Rank {self.rank} è´Ÿè´£ layer {layer_idx}ï¼Œä½† local_cache ä¸º Noneï¼")
            
            # ä»KVCacheå¯¹è±¡æˆ–tupleä¸­æå–keyå’Œvalue
            if isinstance(local_cache, (list, tuple)) and len(local_cache) == 2:
                if hasattr(local_cache[0], 'get_data'):   # è¿™é‡Œçš„åˆ¤æ–­å§‹ç»ˆæ˜¯False
                    # KVCacheå¯¹è±¡
                    key = local_cache[0].get_data()
                    value = local_cache[1].get_data()
                else:
                    # tuple
                    key, value = local_cache
            else:
                raise TypeError(f"Unexpected local_cache type: {type(local_cache)}")
            
            # å¹¿æ’­shape
            shape_info = torch.tensor(key.shape, dtype=torch.long, device='cpu')
            # shape_info = torch.tensor([key.shape[2]], dtype=torch.long, device='cpu')
            for dest_rank in range(self.world_size):
                if dest_rank != self.rank:
                    dist.send(shape_info, dst=dest_rank)
            
            # å¹¿æ’­cacheæ•°æ®
            for dest_rank in range(self.world_size):
                if dest_rank != self.rank:
                    if self.backend == 'gloo':
                        dist.send(key.cpu().contiguous(), dst=dest_rank)
                        dist.send(value.cpu().contiguous(), dst=dest_rank)
                    else:
                        dist.send(key.contiguous(), dst=dest_rank)
                        dist.send(value.contiguous(), dst=dest_rank)
            
            # è¿½åŠ åˆ°è‡ªå·±çš„KVCache
            kv_cache_list[layer_idx][0].cat(key)
            kv_cache_list[layer_idx][1].cat(value)
            
        else:
            # æˆ‘ä¸æ˜¯ownerï¼Œæ¥æ”¶cache
            shape_info = torch.zeros(4, dtype=torch.long, device='cpu')
            dist.recv(shape_info, src=owner_rank)
            kv_shape = tuple(shape_info.tolist())
            
            if self.backend == 'gloo':
                recv_key = torch.zeros(kv_shape, dtype=torch.float16, device='cpu')
                recv_value = torch.zeros(kv_shape, dtype=torch.float16, device='cpu')
                dist.recv(recv_key, src=owner_rank)
                dist.recv(recv_value, src=owner_rank)
                recv_key = recv_key.to(f"cuda:{self.local_device}")
                recv_value = recv_value.to(f"cuda:{self.local_device}")
            else:
                recv_key = torch.zeros(kv_shape, dtype=torch.float16, device=f"cuda:{self.local_device}")
                recv_value = torch.zeros(kv_shape, dtype=torch.float16, device=f"cuda:{self.local_device}")
                dist.recv(recv_key, src=owner_rank)
                dist.recv(recv_value, src=owner_rank)
            
            # è¿½åŠ åˆ°KVCache    
            kv_cache_list[layer_idx][0].cat(recv_key)
            kv_cache_list[layer_idx][1].cat(recv_value)
    
    def _broadcast_layer_cache(self, layer_idx: int, local_cache: Tuple[torch.Tensor, torch.Tensor], chunk_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¹¿æ’­æŸä¸€å±‚çš„cacheåˆ°æ‰€æœ‰rank
        
        é€»è¾‘ï¼š
        - ç¡®å®šå“ªä¸ªrankè´Ÿè´£è¯¥layerï¼ˆowner_rankï¼‰
        - owner_rankå°†local_cacheå¹¿æ’­ç»™æ‰€æœ‰å…¶ä»–rank
        - æ‰€æœ‰rankè¿”å›ç›¸åŒçš„cache
        
        Args:
            layer_idx: å±‚ç´¢å¼•
            local_cache: å½“å‰rankè®¡ç®—çš„è¯¥å±‚cache (key, value)ï¼Œéowner rankä¼ å…¥dummy
            chunk_idx: å½“å‰chunkç´¢å¼•
            
        Returns:
            synced_cache: åŒæ­¥åçš„cacheï¼ˆæ‰€æœ‰rankç›¸åŒï¼‰
        """
        # ç¡®å®šå“ªä¸ªrankè´Ÿè´£è¿™ä¸€å±‚
        num_layers = self.model.config.num_hidden_layers
        layers_per_rank = num_layers // self.world_size
        owner_rank = layer_idx // layers_per_rank
        if owner_rank >= self.world_size:
            owner_rank = self.world_size - 1
        
        if self.rank == owner_rank:
            # æˆ‘æ˜¯ownerï¼Œå…ˆå¹¿æ’­shapeï¼Œå†å¹¿æ’­cacheæ•°æ®
            if local_cache is None:
                raise RuntimeError(f"Rank {self.rank} è´Ÿè´£ layer {layer_idx}ï¼Œä½† local_cache ä¸º Noneï¼")
            
            key, value = local_cache
            
            # å¹¿æ’­shapeä¿¡æ¯åˆ°æ‰€æœ‰å…¶ä»–rank
            shape_info = torch.tensor(key.shape, dtype=torch.long, device='cpu')
            for dest_rank in range(self.world_size):
                if dest_rank != self.rank:
                    dist.send(shape_info, dst=dest_rank)
            
            self.logger.debug(f"    Layer {layer_idx} Chunk {chunk_idx}: å¹¿æ’­cache (shape={key.shape})")
            
            # å¹¿æ’­cacheæ•°æ®
            for dest_rank in range(self.world_size):
                if dest_rank != self.rank:
                    if self.backend == 'gloo':
                        dist.send(key.cpu().contiguous(), dst=dest_rank)
                        dist.send(value.cpu().contiguous(), dst=dest_rank)
                    else:
                        dist.send(key.contiguous(), dst=dest_rank)
                        dist.send(value.contiguous(), dst=dest_rank)
            
            return (key, value)
        else:
            # æˆ‘ä¸æ˜¯ownerï¼Œå…ˆæ¥æ”¶shapeï¼Œå†æ¥æ”¶cacheæ•°æ®
            # æ¥æ”¶shapeä¿¡æ¯
            shape_info = torch.zeros(4, dtype=torch.long, device='cpu')
            dist.recv(shape_info, src=owner_rank)
            
            # å‡†å¤‡æ¥æ”¶ç¼“å†²åŒº
            # batch_size = shape_info[1].item()
            # seq_len = shape_info[2].item()
            # num_heads = shape_info[3].item()
            # head_dim = shape_info[4].item()
            shape_info = tuple(shape_info.tolist())
            if self.backend == 'gloo':
                recv_key = torch.zeros(shape_info, dtype=torch.float16, device='cpu')
                recv_value = torch.zeros(shape_info, dtype=torch.float16, device='cpu')
                dist.recv(recv_key, src=owner_rank)
                dist.recv(recv_value, src=owner_rank)
                # ç§»åˆ°GPU
                recv_key = recv_key.to(f"cuda:{self.local_device}")
                recv_value = recv_value.to(f"cuda:{self.local_device}")
            else:
                recv_key = torch.zeros(shape_info, dtype=torch.float16, device=f"cuda:{self.local_device}")
                recv_value = torch.zeros(shape_info, dtype=torch.float16, device=f"cuda:{self.local_device}")
                # recv_key = torch.zeros(batch_size, num_heads, seq_len, head_dim, dtype=torch.float16, device=f"cuda:{self.local_device}")
                # recv_value = torch.zeros(batch_size, num_heads, seq_len, head_dim, dtype=torch.float16, device=f"cuda:{self.local_device}")
                dist.recv(recv_key, src=owner_rank)
                dist.recv(recv_value, src=owner_rank)
            
            self.logger.debug(f"    Layer {layer_idx} Chunk {chunk_idx}: ä»Rank {owner_rank}æ¥æ”¶cache (shape={recv_key.shape})")
            return (recv_key, recv_value)
    
    def _broadcast_next_token(self, token_id: torch.Tensor = None) -> torch.Tensor:
        """
        å¹¿æ’­next tokenåˆ°æ‰€æœ‰rankï¼Œç¡®ä¿decodeé˜¶æ®µæ‰€æœ‰è®¾å¤‡ä½¿ç”¨ç›¸åŒçš„token
        
        Args:
            token_id: å½“å‰rankç”Ÿæˆçš„token_idï¼Œshape=[batch, 1]
                     åªæœ‰æœ€åä¸€ä¸ªrankéœ€è¦ä¼ å…¥æœ‰æ•ˆå€¼
        
        Returns:
            synced_token_id: åŒæ­¥åçš„tokenï¼Œæ‰€æœ‰rankç›¸åŒ
        """
        if self.rank == self.world_size - 1:
            # æœ€åä¸€ä¸ªrankè´Ÿè´£ç”Ÿæˆtokenå¹¶å¹¿æ’­
            if token_id is None:
                raise RuntimeError("æœ€åä¸€ä¸ªrankçš„token_idä¸èƒ½ä¸ºNone")
            # ç¡®ä¿åœ¨CPUä¸Šä»¥ä¾¿glooåç«¯ä½¿ç”¨
            if self.backend == 'gloo':
                token_cpu = token_id.cpu().contiguous()
                for dest_rank in range(self.world_size - 1):
                    dist.send(token_cpu, dst=dest_rank)
                return token_id
            else:
                # NCCLå¯ä»¥ç›´æ¥åœ¨GPUä¸Šå¹¿æ’­
                token_gpu = token_id.contiguous()
                for dest_rank in range(self.world_size - 1):
                    dist.send(token_gpu, dst=dest_rank)
                return token_id
        else:
            # å…¶ä»–rankæ¥æ”¶token
            if self.backend == 'gloo':
                token_cpu = torch.zeros((1, 1), dtype=torch.long, device='cpu')
                dist.recv(token_cpu, src=self.world_size - 1)
                token_gpu = token_cpu.to(f"cuda:{self.local_device}")
                return token_gpu
            else:
                token_gpu = torch.zeros((1, 1), dtype=torch.long, device=f"cuda:{self.local_device}")
                dist.recv(token_gpu, src=self.world_size - 1)
                return token_gpu
        
    def print_timing_stats(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡"""
        self.logger.info("=" * 60)
        self.logger.info("æ—¶é—´ç»Ÿè®¡:")
        prefill_time = self.timing_stats['prefill_end'] - self.timing_stats['prefill_start']
        cache_sync_time = self.timing_stats['cache_sync_end'] - self.timing_stats['prefill_end']
        decode_time = self.timing_stats['decode_end'] - self.timing_stats['decode_start']
        total_time = self.timing_stats['decode_end'] - self.timing_stats['prefill_start']
        
        self.logger.info(f"  Prefill æ—¶é—´:      {prefill_time:.3f}s")
        self.logger.info(f"  Cache åŒæ­¥æ—¶é—´:    {cache_sync_time:.3f}s")
        self.logger.info(f"  Decode æ—¶é—´:       {decode_time:.3f}s")
        self.logger.info(f"  æ€»æ—¶é—´:           {total_time:.3f}s")
        self.logger.info("=" * 60)
        
    def run_inference(self, prompt: str, max_new_tokens: int = 100) -> str:
        """è¿è¡Œå®Œæ•´çš„æ¨ç†æµç¨‹"""
        try:
            # Prefillé˜¶æ®µ
            last_hidden, kv_caches = self.prefill_phase(prompt)
            
            # Decodeé˜¶æ®µ
            generated_text = self.decode_phase(last_hidden, kv_caches, max_new_tokens)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.print_timing_stats()
            
            if self.rank == 0:
                self.logger.info("=" * 60)
                self.logger.info("ç”Ÿæˆç»“æœ:")
                self.logger.info(generated_text)
                self.logger.info("=" * 60)
            
            return generated_text
            
        except Exception as e:
            self.logger.error(f"æ¨ç†è¿‡ç¨‹å‡ºé”™: {e}", exc_info=True)
            # åˆ·æ–°æ—¥å¿—ç¼“å†²åŒºï¼Œç¡®ä¿é”™è¯¯ä¿¡æ¯è¢«å†™å…¥æ–‡ä»¶
            for handler in self.logger.handlers:
                handler.flush()
            # é€šçŸ¥å…¶ä»– rank å‘ç”Ÿé”™è¯¯ï¼ˆä½¿ç”¨è¶…æ—¶é¿å…æŒ‚èµ·ï¼‰
            try:
                # å°è¯•ä¸€ä¸ªå¿«é€Ÿçš„ barrierï¼Œå¦‚æœå…¶ä»– rank ä¹Ÿåœ¨é”™è¯¯å¤„ç†ä¸­ä¼šå¿«é€Ÿå¤±è´¥
                dist.barrier()
            except Exception:
                pass  # å¿½ç•¥ barrier å¤±è´¥
            raise
        finally:
            self.cleanup()
    
    def _log_cache_received_status(self):
        """æ‰“å°Cacheæ¥æ”¶çŠ¶æ€çŸ©é˜µï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        if self.cache_received_indicator is None:
            return
        
        num_layers = self.cache_received_indicator.shape[1]
        
        self.logger.info("=" * 60)
        self.logger.info("Cacheæ¥æ”¶çŠ¶æ€çŸ©é˜µ (Chunk Ã— Layer):")
        
        # æ‰“å°è¡¨å¤´
        header = "Chunk".ljust(8) + " | " + " ".join([f"L{i}".ljust(2) for i in range(0, num_layers, 2)])
        self.logger.info(header)
        self.logger.info("-" * len(header))
        
        # æ‰“å°æ¯ä¸€è¡Œ
        for chunk_idx in range(self.num_chunks):
            row_str = f"Chunk{chunk_idx}".ljust(8) + " | "
            row_str += " ".join([
                f"{self.cache_received_indicator[chunk_idx, layer_idx].item()}" 
                for layer_idx in range(0, num_layers, 2)
            ])
            self.logger.info(row_str)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_cells = self.num_chunks * num_layers
        received_cells = self.cache_received_indicator.sum().item()
        self.logger.info(f"æ¥æ”¶å®Œæˆåº¦: {received_cells}/{total_cells} ({100*received_cells/total_cells:.1f}%)")
        self.logger.info("=" * 60)
            
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # åˆ·æ–°æ‰€æœ‰æ—¥å¿—
            for handler in self.logger.handlers:
                handler.flush()
            
            # é”€æ¯è¿›ç¨‹ç»„ï¼ˆå¯èƒ½ä¼šå› ä¸ºå…¶ä»– rank å´©æºƒè€Œå¤±è´¥ï¼‰
            if dist.is_initialized():
                try:
                    dist.destroy_process_group()
                except Exception as e:
                    self.logger.warning(f"é”€æ¯è¿›ç¨‹ç»„æ—¶å‡ºé”™ï¼ˆå¯èƒ½å…¶ä»– rank å·²å´©æºƒï¼‰: {e}")
        except Exception as e:
            # ç¡®ä¿ cleanup ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            print(f"Rank {self.rank} cleanup å‡ºé”™: {e}")


def main():
    parser = argparse.ArgumentParser(description='SP+PPåˆ†å¸ƒå¼æ¨ç†')
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--rank', type=int, required=True, help='å½“å‰è®¾å¤‡rank (0, 1, 2)')
    parser.add_argument('--world_size', type=int, default=3, help='æ€»è®¾å¤‡æ•°')
    parser.add_argument('--master_addr', type=str, default='localhost', help='ä¸»èŠ‚ç‚¹åœ°å€')
    parser.add_argument('--master_port', type=str, default='29500', help='ä¸»èŠ‚ç‚¹ç«¯å£')
    parser.add_argument('--chunk_size', type=int, default=128, help='SPçš„chunkå¤§å°')
    parser.add_argument('--sync_strategy', type=str, default='pairwise', 
                       choices=['pairwise', 'ring'], help='CacheåŒæ­¥ç­–ç•¥')
    parser.add_argument('--device_mode', type=str, default='single_node',
                       choices=['single_node', 'multi_node'], 
                       help='è®¾å¤‡æ¨¡å¼ï¼šsingle_node(å•æœºå¤šå¡) æˆ– multi_node(å¤šæœºå•å¡)')
    parser.add_argument('--backend', type=str, default='auto',
                       choices=['auto', 'nccl', 'gloo'],
                       help='é€šä¿¡åç«¯ï¼šauto(è‡ªåŠ¨é€‰æ‹©), nccl(GPUé€šä¿¡), gloo(CPUé€šä¿¡)')
    parser.add_argument('--prompt', type=str, default='è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚', 
                       help='è¾“å…¥prompt')
    parser.add_argument('--max_new_tokens', type=int, default=100, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = DistributedInferenceEngine(
        model_path=args.model_path,
        rank=args.rank,
        world_size=args.world_size,
        master_addr=args.master_addr,
        master_port=args.master_port,
        chunk_size=args.chunk_size,
        sync_strategy=args.sync_strategy,
        device_mode=args.device_mode,
        backend=args.backend
    )
    
    # è¿è¡Œæ¨ç†
    engine.run_inference(args.prompt, args.max_new_tokens)


if __name__ == "__main__":
    main()
