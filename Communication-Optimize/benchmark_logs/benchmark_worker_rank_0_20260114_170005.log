2026-01-14 17:00:05,395 - root - INFO - Worker Rank 0 启动，日志: /data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/benchmark_logs/benchmark_worker_rank_0_20260114_170005.log
2026-01-14 17:00:12,551 - root - INFO - Rank 0 开始测试: mode=p2p, test_id=0, warmup=True
2026-01-14 17:00:12,551 - root - INFO - Rank 0 创建新引擎: mode=p2p
2026-01-14 17:00:12,552 - ZMQ-Rank-0 - INFO - 检测到 CUDA_VISIBLE_DEVICES=5，使用逻辑设备 cuda:0
2026-01-14 17:00:12,552 - ZMQ-Rank-0 - INFO - 初始化ZMQ通信管理器 (mode=p2p)
2026-01-14 17:00:12,553 - ZMQComm-Rank0 - INFO - 设置P2P模式 ZMQ sockets
2026-01-14 17:00:12,553 - ZMQComm-Rank0 - INFO - P2P模式 ZMQ sockets设置完成
2026-01-14 17:00:12,554 - ZMQ-Rank-0 - INFO - 等待其他节点就绪 (0.5秒)...
2026-01-14 17:00:13,055 - ZMQComm-Rank0 - INFO - 接收线程启动
2026-01-14 17:00:13,055 - ZMQComm-Rank0 - INFO - P2P发送线程启动
2026-01-14 17:00:13,055 - ZMQComm-Rank0 - INFO - 通信管理器已启动 (P2PCommManager)
2026-01-14 17:00:13,056 - ZMQ-Rank-0 - INFO - 初始化设备 Rank 0/3
2026-01-14 17:00:13,336 - ZMQ-Rank-0 - INFO - 加载模型: /data/home/chenyu/Coding/SD+SoT/models/Qwen3-4B
2026-01-14 17:00:14,845 - ZMQ-Rank-0 - INFO - 模型加载完成，共36层
2026-01-14 17:00:14,848 - ZMQ-Rank-0 - INFO - KV Cache 预分配完成（最大长度: 2200）
2026-01-14 17:00:14,848 - root - INFO - Rank 0 引擎创建完成
2026-01-14 17:00:14,849 - ZMQ-Rank-0 - INFO - ============================================================
2026-01-14 17:00:14,849 - ZMQ-Rank-0 - INFO - 开始 Prefill 阶段 (SP+PP with ZMQ)
2026-01-14 17:00:14,862 - ZMQ-Rank-0 - INFO - Prompt长度: 17 tokens
2026-01-14 17:00:14,862 - ZMQ-Rank-0 - INFO - 切分为 1 个chunks (chunk_size=128)
2026-01-14 17:00:14,862 - ZMQ-Rank-0 - INFO - 负责层范围: [0, 12)
2026-01-14 17:00:14,862 - ZMQ-Rank-0 - INFO - 处理 chunk 1/1
2026-01-14 17:00:15,080 - ZMQ-Rank-0 - INFO -   chunk 1 处理完成
2026-01-14 17:00:15,759 - ZMQ-Rank-0 - INFO - First token 同步完成: 220
2026-01-14 17:00:15,760 - ZMQ-Rank-0 - INFO - 等待接收 24 个cache...
2026-01-14 17:00:15,763 - ZMQ-Rank-0 - INFO - 所有cache接收完成
2026-01-14 17:00:15,763 - ZMQ-Rank-0 - INFO - Prefill 总耗时: 0.914s, cache同步耗时: 0.914s, 计算耗时: 0.231s
2026-01-14 17:00:15,763 - root - INFO - Rank 0 完成测试: test_id=0
2026-01-14 17:00:15,765 - root - INFO - Rank 0 开始测试: mode=p2p, test_id=0, warmup=False
2026-01-14 17:00:15,765 - ZMQ-Rank-0 - INFO - ============================================================
2026-01-14 17:00:15,765 - ZMQ-Rank-0 - INFO - 开始 Prefill 阶段 (SP+PP with ZMQ)
2026-01-14 17:00:15,766 - ZMQ-Rank-0 - INFO - Prompt长度: 17 tokens
2026-01-14 17:00:15,766 - ZMQ-Rank-0 - INFO - 切分为 1 个chunks (chunk_size=128)
2026-01-14 17:00:15,766 - ZMQ-Rank-0 - INFO - 负责层范围: [0, 12)
2026-01-14 17:00:15,766 - ZMQ-Rank-0 - INFO - 处理 chunk 1/1
2026-01-14 17:00:15,820 - ZMQ-Rank-0 - INFO -   chunk 1 处理完成
2026-01-14 17:00:15,945 - ZMQ-Rank-0 - INFO - First token 同步完成: 220
2026-01-14 17:00:15,946 - ZMQ-Rank-0 - INFO - 等待接收 24 个cache...
2026-01-14 17:00:15,950 - ZMQ-Rank-0 - INFO - 所有cache接收完成
2026-01-14 17:00:15,950 - ZMQ-Rank-0 - INFO - Prefill 总耗时: 0.185s, cache同步耗时: 0.185s, 计算耗时: 0.056s
2026-01-14 17:00:15,950 - root - INFO - Rank 0 完成测试: test_id=0
2026-01-14 17:00:25,961 - root - INFO - Rank 0 开始测试: mode=ring, test_id=0, warmup=True
2026-01-14 17:00:25,962 - root - INFO - Rank 0 切换模式: p2p -> ring，清理旧引擎
2026-01-14 17:00:26,029 - ZMQComm-Rank0 - INFO - P2P发送线程退出
2026-01-14 17:00:26,059 - ZMQComm-Rank0 - INFO - 接收线程退出
2026-01-14 17:00:26,060 - ZMQComm-Rank0 - INFO - 通信管理器已停止
2026-01-14 17:00:26,257 - ZMQ-Rank-0 - INFO - 资源清理完成
2026-01-14 17:00:26,365 - root - INFO - Rank 0 等待5秒让资源完全释放...
2026-01-14 17:00:31,371 - root - INFO - Rank 0 创建新引擎: mode=ring
2026-01-14 17:00:31,374 - ZMQ-Rank-0 - INFO - 检测到 CUDA_VISIBLE_DEVICES=5，使用逻辑设备 cuda:0
2026-01-14 17:00:31,374 - ZMQ-Rank-0 - INFO - 初始化ZMQ通信管理器 (mode=ring)
2026-01-14 17:00:31,375 - ZMQComm-Rank0 - INFO - 设置Ring模式 ZMQ sockets
2026-01-14 17:00:31,375 - ZMQComm-Rank0 - INFO - Ring模式 ZMQ sockets设置完成 (prev=2, next=1)
2026-01-14 17:00:31,375 - ZMQComm-Rank0 - INFO - Ring拓扑: prev=2 <- rank0 -> next=1
2026-01-14 17:00:31,375 - ZMQ-Rank-0 - INFO - 等待其他节点就绪 (0.5秒)...
2026-01-14 17:00:31,876 - ZMQComm-Rank0 - INFO - 接收线程启动
2026-01-14 17:00:31,876 - ZMQComm-Rank0 - INFO - Ring发送线程启动
2026-01-14 17:00:31,876 - ZMQComm-Rank0 - INFO - 通信管理器已启动 (RingCommManager)
2026-01-14 17:00:31,876 - ZMQ-Rank-0 - INFO - 初始化设备 Rank 0/3
2026-01-14 17:00:32,163 - ZMQ-Rank-0 - INFO - 加载模型: /data/home/chenyu/Coding/SD+SoT/models/Qwen3-4B
2026-01-14 17:00:32,735 - root - ERROR - Rank 0 测试出错: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 11.25 MiB is free. Process 2087597 has 912.23 MiB memory in use. Process 579772 has 12.94 GiB memory in use. Including non-PyTorch memory, this process has 9.62 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 148.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_performance_benchmark.py", line 160, in benchmark_worker
    current_engine = ZMQDistributedInferenceEngine(
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_distributed_inference.py", line 98, in __init__
    self.model = self._load_model()
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_distributed_inference.py", line 142, in _load_model
    model = Qwen3ForCausalLM.from_pretrained(
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/transformers/modeling_utils.py", line 5048, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/transformers/modeling_utils.py", line 5468, in _load_pretrained_model
    _error_msgs, disk_offload_index = load_shard_file(args)
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/transformers/modeling_utils.py", line 843, in load_shard_file
    disk_offload_index = _load_state_dict_into_meta_model(
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/transformers/modeling_utils.py", line 750, in _load_state_dict_into_meta_model
    param = param.to(casting_dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 11.25 MiB is free. Process 2087597 has 912.23 MiB memory in use. Process 579772 has 12.94 GiB memory in use. Including non-PyTorch memory, this process has 9.62 GiB memory in use. Of the allocated memory 9.03 GiB is allocated by PyTorch, and 148.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-01-14 17:00:33,245 - root - INFO - Rank 0 开始测试: mode=ring, test_id=0, warmup=False
2026-01-14 17:00:33,247 - root - INFO - Rank 0 创建新引擎: mode=ring
2026-01-14 17:00:33,249 - ZMQ-Rank-0 - INFO - 检测到 CUDA_VISIBLE_DEVICES=5，使用逻辑设备 cuda:0
2026-01-14 17:00:33,251 - ZMQ-Rank-0 - INFO - 初始化ZMQ通信管理器 (mode=ring)
2026-01-14 17:00:33,251 - ZMQComm-Rank0 - INFO - 设置Ring模式 ZMQ sockets
2026-01-14 17:00:33,252 - root - ERROR - Rank 0 测试出错: Address already in use (addr='tcp://*:45006')
Traceback (most recent call last):
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_performance_benchmark.py", line 160, in benchmark_worker
    current_engine = ZMQDistributedInferenceEngine(
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_distributed_inference.py", line 79, in __init__
    self.comm = create_zmq_comm_manager(
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_comm_manager.py", line 1142, in create_zmq_comm_manager
    manager = RingCommManager(
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_comm_manager.py", line 885, in __init__
    super().__init__(*args, **kwargs)
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_comm_manager.py", line 406, in __init__
    self._setup_sockets()
  File "/data/home/chenyu/Coding/SD+SoT/Speculative-Decoding-Enabled-Skeleton-of-Thought/Communication-Optimize/zmq_comm_manager.py", line 914, in _setup_sockets
    recv_socket.bind(f"tcp://*:{recv_port}")
  File "/data/home/chenyu/anaconda3/envs/sdsot/lib/python3.9/site-packages/zmq/sugar/socket.py", line 320, in bind
    super().bind(addr)
  File "zmq/backend/cython/_zmq.py", line 1009, in zmq.backend.cython._zmq.Socket.bind
  File "zmq/backend/cython/_zmq.py", line 190, in zmq.backend.cython._zmq._check_rc
zmq.error.ZMQError: Address already in use (addr='tcp://*:45006')
2026-01-14 17:00:33,254 - root - INFO - Rank 0 收到停止信号
2026-01-14 17:00:33,254 - root - INFO - Rank 0 退出
