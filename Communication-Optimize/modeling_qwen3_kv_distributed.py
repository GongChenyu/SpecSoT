"""
åˆ†å¸ƒå¼ç‰ˆæœ¬çš„Qwen3æ¨¡å‹
æ”¯æŒPipeline Parallel (PP) å’Œ KV CacheåŒæ­¥
åŸºäºmodeling_qwen3_kv.pyä¿®æ”¹
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple, List
from transformers.modeling_outputs import CausalLMOutputWithPast, BaseModelOutputWithPast
from modeling_qwen3_kv import (
    Qwen3ForCausalLM,
    Qwen3Model,
    Qwen3Config,
    Qwen3PreTrainedModel
)
from cache_sync_manager import create_sync_manager, CacheSyncManager
from kv_cache import KVCache, initialize_past_key_values


class Qwen3ModelDistributed(Qwen3Model):
    """æ”¯æŒPipeline Parallelçš„Qwen3Model"""
    
    def __init__(self, config: Qwen3Config, rank: int = 0, world_size: int = 1, sync_strategy: str = "pairwise", backend: str = "nccl"):
        super().__init__(config)
        
        self.rank = rank
        self.world_size = world_size
        self.sync_strategy = sync_strategy
        self.backend = backend
        
        # Pipeline Parallelé…ç½®
        self.pp_start_layer = 0
        self.pp_end_layer = config.num_hidden_layers
        self.is_pipeline_mode = False  # æ˜¯å¦å¤„äºpipelineæ¨¡å¼
        
        # CacheåŒæ­¥ç®¡ç†å™¨
        if world_size > 1:
            self.cache_sync_manager = create_sync_manager(
                rank=rank,
                world_size=world_size,
                strategy=sync_strategy,
                streaming=False,
                backend=backend
            )
        else:
            self.cache_sync_manager = None
        
        # ğŸ†• åˆå§‹åŒ–KV Cacheï¼ˆé¢„åˆ†é…è¿ç»­å†…å­˜ï¼‰
        self.past_key_values = None
        self.past_key_values_data = None
        self.current_length_data = None
        self._kv_cache_initialized = False
            
    def set_pipeline_range(self, start_layer: int, end_layer: int):
        """è®¾ç½®å½“å‰rankè´Ÿè´£çš„å±‚èŒƒå›´"""
        self.pp_start_layer = start_layer
        self.pp_end_layer = end_layer
        self.is_pipeline_mode = True
        
    def set_full_model_mode(self):
        """è®¾ç½®ä¸ºå…¨é‡æ¨¡å‹æ¨¡å¼ï¼ˆdecodeé˜¶æ®µä½¿ç”¨ï¼‰"""
        self.pp_start_layer = 0
        self.pp_end_layer = self.config.num_hidden_layers
        self.is_pipeline_mode = False
    
    def initialize_kv_cache(self, max_length: int = 2200, batch_size: int = 1):
        """åˆå§‹åŒ–é¢„åˆ†é…çš„KV Cacheï¼ˆè¿ç»­å†…å­˜ï¼‰"""
        if not self._kv_cache_initialized:
            # ä¸´æ—¶åŒ…è£…æ¨¡å‹ä¾› initialize_past_key_values ä½¿ç”¨
            class ModelWrapper:
                def __init__(self, model, config):
                    self.model = model
                    self.config = config
                    self.dtype = next(model.parameters()).dtype
                    self.layers = model.layers
            
            wrapper = ModelWrapper(self, self.config)
            self.past_key_values, self.past_key_values_data, self.current_length_data = \
                initialize_past_key_values(wrapper, max_length=max_length, batch_size=batch_size)
            
            self._kv_cache_initialized = True
            
            # æ‰“å°åˆå§‹åŒ–ä¿¡æ¯
            total_size_gb = sum(d.numel() * d.element_size() for d in self.past_key_values_data) / (1024**3)
            print(f"[Rank {self.rank}] é¢„åˆ†é…KV Cache:")
            print(f"  å±‚æ•°: {self.config.num_hidden_layers}")
            print(f"  æœ€å¤§é•¿åº¦: {max_length}")
            print(f"  æ€»å¤§å°: {total_size_gb:.2f} GB")
            print(f"  ç¼“å†²åŒºæ•°é‡: {len(self.past_key_values_data)}")
    
    def reset_kv_cache(self):
        """é‡ç½®KV Cacheï¼ˆç”¨äºæ–°çš„æ¨ç†ï¼‰"""
        if self._kv_cache_initialized:
            for layer_caches in self.past_key_values:
                for cache in layer_caches:
                    cache.reset()
            # ä¹Ÿé‡ç½®lengthæ•°æ®
            if self.current_length_data is not None:
                self.current_length_data.zero_()
        
    def forward_pipeline_stage(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor]]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ) -> dict:
        """
        Pipeline stageçš„forward
        åªè®¡ç®—å½“å‰rankè´Ÿè´£çš„å±‚
        
        Args:
            input_ids: è¾“å…¥token ids (åªæœ‰ç¬¬ä¸€ä¸ªrankéœ€è¦)
            hidden_states: ä»ä¸Šä¸€ä¸ªrankæ¥æ”¶çš„hidden states (éç¬¬ä¸€ä¸ªrankéœ€è¦)
            å…¶ä»–å‚æ•°åŒQwen3Model.forward
            
        Returns:
            dictåŒ…å«:
                - last_hidden_state: æœ€åä¸€å±‚çš„hidden state
                - past_key_values: å½“å‰rankè®¡ç®—çš„å±‚çš„KV cache
        """
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        
        batch_size = input_ids.shape[0] if input_ids is not None else hidden_states.shape[0]
        seq_length = input_ids.shape[1] if input_ids is not None else hidden_states.shape[1]
        
        # ç¬¬ä¸€ä¸ªrank: ä»input_idsè·å–embeddings
        if self.rank == 0 and input_ids is not None:
            hidden_states = self.embed_tokens(input_ids)
        elif hidden_states is None:
            raise ValueError("éç¬¬ä¸€ä¸ªrankå¿…é¡»æä¾›hidden_states")
            
        # å‡†å¤‡attention maskå’Œposition ids
        past_key_values_length = 0
        if past_key_values is not None and len(past_key_values) > 0:
            past_key_values_length = past_key_values[0][0].shape[2]
            
        seq_length_with_past = seq_length + past_key_values_length
        
        if position_ids is None:
            device = hidden_states.device
            position_ids = torch.arange(
                past_key_values_length,
                seq_length + past_key_values_length,
                dtype=torch.long,
                device=device,
            )
            position_ids = position_ids.unsqueeze(0)
            
        if attention_mask is None:
            attention_mask = torch.ones(
                (batch_size, seq_length_with_past),
                dtype=torch.bool,
                device=hidden_states.device,
            )
            
        attention_mask = self._prepare_decoder_attention_mask(
            attention_mask,
            (batch_size, seq_length),
            hidden_states,
            past_key_values_length,
        )
        
        # åˆ›å»ºposition embeddings
        position_embeddings = self.rotary_emb(hidden_states, position_ids)
        
        # åªå¤„ç†å½“å‰rankè´Ÿè´£çš„å±‚
        current_kv_caches = []
        
        for layer_idx in range(self.pp_start_layer, self.pp_end_layer):
            decoder_layer = self.layers[layer_idx]
            
            # è·å–è¯¥å±‚çš„past_key_value
            layer_past_kv = None
            if past_key_values is not None:
                # past_key_valueså¯èƒ½åªåŒ…å«éƒ¨åˆ†å±‚çš„cache
                local_idx = layer_idx - self.pp_start_layer
                if local_idx < len(past_key_values):
                    layer_past_kv = past_key_values[local_idx]
            
            layer_outputs = decoder_layer(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=layer_past_kv,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs
            )
            
            hidden_states = layer_outputs[0]
            
            if use_cache:
                # layer_outputsæ ¼å¼: (hidden_states,) or (hidden_states, attn_weights,) or (hidden_states, past_kv) or (hidden_states, attn_weights, past_kv)
                # å¦‚æœoutput_attentions=Trueä¸”use_cache=True: (hidden_states, attn_weights, past_kv)
                # å¦‚æœoutput_attentions=Falseä¸”use_cache=True: (hidden_states, past_kv)
                kv_idx = 2 if output_attentions else 1
                current_kv_caches.append(layer_outputs[kv_idx])
        
        # æœ€åä¸€ä¸ªrankéœ€è¦è¿‡norm
        if self.rank == self.world_size - 1:
            hidden_states = self.norm(hidden_states)
        
        return {
            'last_hidden_state': hidden_states,
            'past_key_values': current_kv_caches if use_cache else None
        }
        
    def sync_kv_caches(self, kv_caches: List[Tuple[torch.Tensor, torch.Tensor]]) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """
        åŒæ­¥æ‰€æœ‰è®¾å¤‡çš„KV cache
        
        Args:
            kv_caches: å½“å‰è®¾å¤‡çš„KV cacheåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å®Œæ•´KV cacheåˆ—è¡¨
        """
        if self.cache_sync_manager is None or self.world_size == 1:
            return kv_caches
            
        return self.cache_sync_manager.sync_all_layers_sync(kv_caches)
    
    def forward_single_layer(
        self,
        layer_idx: int,
        hidden_states: torch.FloatTensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs
    ) -> dict:
        """
        å•å±‚forwardè®¡ç®—ï¼ˆä½¿ç”¨é¢„åˆ†é…çš„KVCacheï¼‰
        
        Args:
            layer_idx: å±‚ç´¢å¼•ï¼ˆå…¨å±€ç´¢å¼•ï¼‰
            hidden_states: è¾“å…¥çš„hidden states
            past_key_value: è¯¥å±‚çš„past KV cacheï¼ˆå¦‚æœä½¿ç”¨KVCacheï¼Œåˆ™å¿½ç•¥ï¼‰
            å…¶ä»–å‚æ•°åŒæ ‡å‡†forward
            
        Returns:
            dictåŒ…å«:
                - hidden_states: è¾“å‡ºçš„hidden states
                - past_key_value: è¯¥å±‚çš„KV cache (è¿ç»­å†…å­˜)
        """
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        
        # ğŸ†• å¦‚æœä½¿ç”¨KVCacheï¼Œä»é¢„åˆ†é…ç¼“å†²åŒºè·å–past_key_value
        if self._kv_cache_initialized and use_cache:
            # è·å–å½“å‰å±‚çš„KVCacheå¯¹è±¡
            layer_kv_cache = self.past_key_values[layer_idx]
            # æ„å»ºpast_key_value tuple
            if layer_kv_cache[0].current_length.item() > 0:
                past_key_value = (
                    layer_kv_cache[0].get_data(),  # key cache
                    layer_kv_cache[1].get_data()   # value cache
                )
            else:
                past_key_value = None
        
        # è·å–å¯¹åº”çš„decoder layer
        decoder_layer = self.layers[layer_idx]
        
        # æ‰§è¡Œè¯¥å±‚çš„forward
        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs
        )
        
        # layer_outputsæ ¼å¼: (hidden_states,) or (hidden_states, past_kv) or (hidden_states, attn_weights, past_kv)
        new_hidden_states = layer_outputs[0]
        
        result = {'hidden_states': new_hidden_states}
        
        if use_cache:
            kv_idx = 2 if output_attentions else 1
            new_kv_cache = layer_outputs[kv_idx] if len(layer_outputs) > kv_idx else None
            
            # ğŸ†• å¦‚æœä½¿ç”¨KVCacheï¼Œå°†æ–°ç”Ÿæˆçš„cacheè¿½åŠ åˆ°é¢„åˆ†é…ç¼“å†²åŒº
            if self._kv_cache_initialized and new_kv_cache is not None:
                new_key, new_value = new_kv_cache
                layer_kv_cache = self.past_key_values[layer_idx]
                
                # ä½¿ç”¨catæ–¹æ³•è¿½åŠ ï¼ˆè‡ªåŠ¨ç®¡ç†è¿ç»­å†…å­˜ï¼‰
                updated_key = layer_kv_cache[0].cat(new_key, dim=2)
                updated_value = layer_kv_cache[1].cat(new_value, dim=2)
                
                # è¿”å›è¿ç»­çš„cacheï¼ˆä»é¢„åˆ†é…ç¼“å†²åŒºï¼‰
                result['past_key_value'] = (updated_key, updated_value)
            else:
                result['past_key_value'] = new_kv_cache
        else:
            result['past_key_value'] = None
            
        return result
    
    def sync_single_layer_cache(
        self,
        layer_idx: int,
        kv_cache: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åŒæ­¥å•å±‚çš„KV cache
        
        åœ¨PPæ¨¡å¼ä¸‹ï¼ŒåŒæ­¥æ˜¯ä¸ºäº†è®©æ¯ä¸ªrankéƒ½è·å–åˆ°å…¶ä»–rankè´Ÿè´£å±‚çš„cacheã€‚
        ç”±äºcache_sync_managerä¼šåœ¨sequenceç»´åº¦æ‹¼æ¥ï¼Œæ‰€ä»¥åŒæ­¥åçš„cacheå¯èƒ½å˜å¤§ã€‚
        æˆ‘ä»¬ç›´æ¥è¿”å›åŒæ­¥åçš„cacheï¼Œä¸å†™å›é¢„åˆ†é…ç¼“å†²åŒºï¼ˆé¢„åˆ†é…ç¼“å†²åŒºåªç”¨äºå•ä¸ªrankçš„æœ¬åœ°cacheç®¡ç†ï¼‰ã€‚
        
        Args:
            layer_idx: å±‚ç´¢å¼•
            kv_cache: å•å±‚çš„(key, value) cache tuple
            
        Returns:
            åŒæ­¥åçš„(key, value) cache tuple
        """
        if self.cache_sync_manager is None or self.world_size == 1:
            return kv_cache
        
        # ç¡®ä¿cacheæ˜¯è¿ç»­çš„ï¼ˆå¦‚æœæ˜¯narrowçš„ç»“æœï¼Œéœ€è¦contiguousï¼‰
        key_cache, value_cache = kv_cache
        if not key_cache.is_contiguous():
            key_cache = key_cache.contiguous()
        if not value_cache.is_contiguous():
            value_cache = value_cache.contiguous()
        
        # ä½¿ç”¨sync_all_layers_syncï¼Œç›´æ¥è¿”å›åŒæ­¥ç»“æœ
        # æ³¨æ„ï¼šcache_sync_managerä¼šåœ¨sequenceç»´åº¦æ‹¼æ¥ï¼Œè¿™åœ¨SP+PPæ··åˆæ¨¡å¼ä¸‹æ˜¯åˆç†çš„
        synced_caches = self.cache_sync_manager.sync_all_layers_sync([(key_cache, value_cache)])
        
        return synced_caches[0]


class Qwen3ForCausalLMDistributed(Qwen3ForCausalLM):
    """æ”¯æŒåˆ†å¸ƒå¼æ¨ç†çš„Qwen3ForCausalLM"""
    
    def __init__(self, config, rank: int = 0, world_size: int = 1, sync_strategy: str = "pairwise", backend: str = "nccl"):
        # è°ƒç”¨çˆ¶ç±»çš„__init__ï¼Œä½†éœ€è¦æ›¿æ¢model
        Qwen3PreTrainedModel.__init__(self, config)
        
        # ä½¿ç”¨åˆ†å¸ƒå¼ç‰ˆæœ¬çš„model
        self.model = Qwen3ModelDistributed(config, rank=rank, world_size=world_size, sync_strategy=sync_strategy, backend=backend)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        
        self.rank = rank
        self.world_size = world_size
        
        # Initialize weights and apply final processing
        self.post_init()
        
    def set_pipeline_range(self, start_layer: int, end_layer: int):
        """è®¾ç½®Pipeline Parallelçš„å±‚èŒƒå›´"""
        self.model.set_pipeline_range(start_layer, end_layer)
        
    def set_full_model_mode(self):
        """è®¾ç½®ä¸ºå…¨é‡æ¨¡å‹æ¨¡å¼"""
        self.model.set_full_model_mode()
        
    def forward_pipeline_stage(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[Tuple[torch.Tensor]]] = None,
        use_cache: Optional[bool] = None,
        **kwargs
    ) -> dict:
        """
        Pipeline stageçš„forward
        
        Returns:
            dictåŒ…å«:
                - last_hidden_state: æœ€åä¸€å±‚çš„hidden state
                - past_key_values: KV cache
                - logits: å¦‚æœæ˜¯æœ€åä¸€ä¸ªrankï¼Œè¿˜åŒ…å«logits
        """
        outputs = self.model.forward_pipeline_stage(
            input_ids=input_ids,
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **kwargs
        )
        
        result = {
            'last_hidden_state': outputs['last_hidden_state'],
            'past_key_values': outputs['past_key_values']
        }
        
        # åªæœ‰æœ€åä¸€ä¸ªrankè®¡ç®—logits
        if self.rank == self.world_size - 1:
            logits = self.lm_head(outputs['last_hidden_state'])
            result['logits'] = logits
            
        return result
        
    def sync_kv_caches(self, kv_caches: List[Tuple[torch.Tensor, torch.Tensor]]) -> List[Tuple[torch.Tensor, torch.Tensor]]:
        """åŒæ­¥KV cache"""
        return self.model.sync_kv_caches(kv_caches)
    
    def forward_single_layer(
        self,
        layer_idx: int,
        hidden_states: torch.FloatTensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: Optional[bool] = None,
        **kwargs
    ) -> dict:
        """å•å±‚forwardï¼Œç›´æ¥è°ƒç”¨modelçš„æ–¹æ³•"""
        return self.model.forward_single_layer(
            layer_idx=layer_idx,
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            position_embeddings=position_embeddings,
            past_key_value=past_key_value,
            use_cache=use_cache,
            **kwargs
        )
    
    def sync_single_layer_cache(
        self,
        layer_idx: int,
        kv_cache: Tuple[torch.Tensor, torch.Tensor]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """åŒæ­¥å•å±‚KV cache"""
        return self.model.sync_single_layer_cache(layer_idx, kv_cache)

